{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unbalanced_LSAPComp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilv7/My_Wireless_Projects/blob/master/Unbalanced_LSAPComp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BKmYW1vIpsn",
        "outputId": "9612aa32-64d0-4129-a042-3a858dbd0a5d"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "!pip3 install hungarian-algorithm\n",
        "from hungarian_algorithm import algorithm\n",
        "!pip install munkres\n",
        "from munkres import Munkres, print_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations \n",
        "!pip install pyyaml h5py\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hungarian-algorithm\n",
            "  Downloading https://files.pythonhosted.org/packages/08/e0/117a3f5b9469fa1f74adb7810c465776e2d6145189fbb7bcaa02b5f7c95f/hungarian_algorithm-0.1.11-py3-none-any.whl\n",
            "Installing collected packages: hungarian-algorithm\n",
            "Successfully installed hungarian-algorithm-0.1.11\n",
            "Collecting munkres\n",
            "  Downloading https://files.pythonhosted.org/packages/90/ab/0301c945a704218bc9435f0e3c88884f6b19ef234d8899fb47ce1ccfd0c9/munkres-1.1.4-py2.py3-none-any.whl\n",
            "Installing collected packages: munkres\n",
            "Successfully installed munkres-1.1.4\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtUla7qedtmp",
        "outputId": "badb1eb2-cce5-4cc5-b16e-6426f2ee28a7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlYrML2bOUTY"
      },
      "source": [
        "noSU = 4\n",
        "noChannels = 4\n",
        "prec = 10000\n",
        "channelindices = np.arange(noChannels)\n",
        "n = max(noSU,noChannels)\n",
        "noTrainingExamples = 50000\n",
        "trainDataIn = 10*np.random.randint(1,prec,[noTrainingExamples,noChannels,noSU])/prec\n",
        "decisionOut = np.zeros([noTrainingExamples,noChannels,noSU])\n",
        "lowestcost = np.zeros(noTrainingExamples)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc61FIAsAKBp"
      },
      "source": [
        "DataSet Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1A8rAo72Yg4"
      },
      "source": [
        "def squarify(M,MODE):\n",
        "    (T,a,b)=M.shape\n",
        "    if a>b:\n",
        "        padding=((0,0),(0,0),(0,a-b))\n",
        "    else:\n",
        "        padding=((0,0),(0,b-a),(0,0))\n",
        "    if (MODE == 'maximum'):\n",
        "      return np.pad(M,padding,mode=MODE)\n",
        "    elif (MODE == 'zero'):\n",
        "      return np.pad(M,padding,mode='constant',constant_values=0)\n",
        "    if (MODE == 'minimum'):\n",
        "      return np.pad(M,padding,mode=MODE)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xRpA7NoGHaK"
      },
      "source": [
        "def decisionGen(G):\n",
        "  matrix = G.tolist()\n",
        "  m = Munkres()\n",
        "  indexes = m.compute(matrix)\n",
        "  decision = np.zeros(G.shape)\n",
        "  totalcost = 0\n",
        "  for row, column in indexes:\n",
        "    decision[row][column] = 1\n",
        "    totalcost = totalcost + matrix[row][column]\n",
        "  return decision,totalcost"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KErk8Iv7yG7k"
      },
      "source": [
        "def hillClimb(G):\n",
        "  X = np.identity(min(noSU,noChannels))\n",
        "  initState = np.take(X,np.random.permutation(X.shape[0]),axis=0,out=X)\n",
        "  initCost = np.sum(G * initState)\n",
        "  currState = initState\n",
        "  currCost = initCost\n",
        "  ctr = 0\n",
        "  while (ctr < 1*noChannels):\n",
        "    i,j = random.choices(np.arange(noChannels),k=2)\n",
        "    currState[[j,i]]=currState[[i,j]]\n",
        "    ctr = ctr + 1\n",
        "    if (np.sum(currState*G))<currCost :\n",
        "      currCost = np.sum(currState*G)\n",
        "      ctr = 0\n",
        "    else :\n",
        "      currState[[j,i]]=currState[[i,j]]\n",
        "  \n",
        "  return currState, currCost"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIJy54MzAJoJ"
      },
      "source": [
        "decidedCost = np.zeros(noTrainingExamples)\n",
        "maxCost = np.zeros(noTrainingExamples)\n",
        "hillCost = np.zeros(noTrainingExamples)\n",
        "hilldec = np.zeros([noTrainingExamples,noChannels,noSU])\n",
        "wdecisionOut = np.zeros([noTrainingExamples,noChannels,noSU])\n",
        "percentagedev = np.zeros(noTrainingExamples)\n",
        "tochill = time.time()\n",
        "for idx in range(noTrainingExamples):\n",
        "  decisionOut[idx,:,:],decidedCost[idx] = decisionGen(trainDataIn[idx,:,:])\n",
        "  wdecisionOut[idx,:,:],_ = decisionGen(np.max(trainDataIn[idx,:,:])-trainDataIn[idx,:,:])\n",
        "  maxCost[idx] = np.sum(wdecisionOut[idx,:,:]*trainDataIn[idx,:,:])\n",
        "\n",
        "  hilldec[idx,:,:],hillCost[idx] = hillClimb(trainDataIn[idx,:,:])\n",
        "  percentagedev[idx] = (100/(maxCost[idx]-decidedCost[idx])) * (hillCost[idx] - decidedCost[idx])\n",
        "tichill = time.time()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHYvkzJZnJ7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b69746-afd2-4178-f97c-bd902c244f73"
      },
      "source": [
        "avgdevhill = np.mean(percentagedev)\n",
        "print(avgdevhill, (tichill-tochill)/noTrainingExamples)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17.988421524011173 0.0004264035940170288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqXo55AGiK9o"
      },
      "source": [
        "CNN model for LSAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfc8jWn0zJzf"
      },
      "source": [
        "#ORIGINAL\n",
        "cnnLayers = {}\n",
        "Model = {}\n",
        "Logs = {}\n",
        "inputs = keras.Input(shape=(noChannels,noSU,1),name='CostMatrix',dtype = tf.float32)\n",
        "\n",
        "for i in range(noSU):\n",
        "   \n",
        "  cnnLayers['L'+str(i)] = layers.Conv2D(4,(1,1),kernel_initializer='random_normal',kernel_regularizer='l2', activation='relu')(inputs)\n",
        "  cnnLayers['L'+str(i)] = layers.Conv2D(8,(1,1),kernel_initializer='glorot_uniform',kernel_regularizer='l2',activation='relu')(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Flatten()(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Dense(128,activation='relu',kernel_regularizer='l2',)(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Dense(256,activation='relu',kernel_regularizer='l2',)(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Dense(64,activation='relu',kernel_regularizer='l2',)(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['Out'+str(i)] = layers.Dense(noChannels,kernel_regularizer='l2',activation='softmax')(cnnLayers['L'+str(i)])\n",
        "  \n",
        "  Model['model'+ str(i)] = keras.Model(inputs,cnnLayers['Out'+str(i)],name ='cnn'+str(i))\n",
        "  Model['model'+ str(i)].compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(lr=0.0001),metrics= ['accuracy'])\n",
        "\n",
        "locals().update(Model)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DoqETVKAhwKw",
        "outputId": "4cf8ac2e-e3df-46a3-c5dc-b9f18727c296"
      },
      "source": [
        "cnnLayers = {}\n",
        "Model = {}\n",
        "Logs = {}\n",
        "inputs = keras.Input(shape=(noChannels,noSU,1),name='CostMatrix',dtype = tf.float32)\n",
        "\n",
        "for i in range(noSU):\n",
        "   \n",
        "  cnnLayers['L'+str(i)] = layers.Conv2D(4,(1,1),kernel_initializer='random_normal',kernel_regularizer='l2', activation='relu')(inputs)\n",
        "  cnnLayers['L'+str(i)] = layers.Conv2D(8,(1,1),kernel_initializer='glorot_uniform',kernel_regularizer='l2',activation='relu')(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Flatten()(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Dense(128,activation='relu',kernel_regularizer='l2',)(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Dense(256,activation='relu',kernel_regularizer='l2',)(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Dense(64,activation='relu',kernel_regularizer='l2',)(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['Out'+str(i)] = layers.Dense(noChannels,kernel_regularizer='l2',activation='softmax')(cnnLayers['L'+str(i)])\n",
        "  \n",
        "  Model['model'+ str(i)] = keras.Model(inputs,cnnLayers['Out'+str(i)],name ='cnn'+str(i))\n",
        "  Model['model'+ str(i)].compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(lr=0.0001),metrics= ['accuracy'])\n",
        "\n",
        "locals().update(Model)\n",
        "\n",
        "for j in range(noSU):\n",
        "  Logs['log'+str(j)] = Model['model'+str(j)].fit(10*trainDataIn,decisionOut[:,:,j],epochs=500,batch_size=1024,verbose=1,validation_split=0.2)\n",
        "\n",
        "locals().update(Logs)\n",
        "fig,axes= plt.subplots(1,2,figsize=(10,4))\n",
        "axes[0].plot(log1.history['accuracy'])\n",
        "axes[1].plot(log1.history['loss'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 313/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3503 - accuracy: 0.9522 - val_loss: 0.3818 - val_accuracy: 0.9355\n",
            "Epoch 314/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.9530 - val_loss: 0.3835 - val_accuracy: 0.9331\n",
            "Epoch 315/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3506 - accuracy: 0.9527 - val_loss: 0.3805 - val_accuracy: 0.9356\n",
            "Epoch 316/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3494 - accuracy: 0.9520 - val_loss: 0.3797 - val_accuracy: 0.9350\n",
            "Epoch 317/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3511 - accuracy: 0.9515 - val_loss: 0.3798 - val_accuracy: 0.9343\n",
            "Epoch 318/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3509 - accuracy: 0.9497 - val_loss: 0.3816 - val_accuracy: 0.9338\n",
            "Epoch 319/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3519 - accuracy: 0.9501 - val_loss: 0.3783 - val_accuracy: 0.9350\n",
            "Epoch 320/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3503 - accuracy: 0.9509 - val_loss: 0.3783 - val_accuracy: 0.9363\n",
            "Epoch 321/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3525 - accuracy: 0.9489 - val_loss: 0.3861 - val_accuracy: 0.9301\n",
            "Epoch 322/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3536 - accuracy: 0.9481 - val_loss: 0.3777 - val_accuracy: 0.9357\n",
            "Epoch 323/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3529 - accuracy: 0.9488 - val_loss: 0.3862 - val_accuracy: 0.9305\n",
            "Epoch 324/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3520 - accuracy: 0.9490 - val_loss: 0.3873 - val_accuracy: 0.9309\n",
            "Epoch 325/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3523 - accuracy: 0.9500 - val_loss: 0.3768 - val_accuracy: 0.9375\n",
            "Epoch 326/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3454 - accuracy: 0.9530 - val_loss: 0.3811 - val_accuracy: 0.9332\n",
            "Epoch 327/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3483 - accuracy: 0.9518 - val_loss: 0.3773 - val_accuracy: 0.9361\n",
            "Epoch 328/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3501 - accuracy: 0.9513 - val_loss: 0.3878 - val_accuracy: 0.9275\n",
            "Epoch 329/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3512 - accuracy: 0.9506 - val_loss: 0.3758 - val_accuracy: 0.9367\n",
            "Epoch 330/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3439 - accuracy: 0.9542 - val_loss: 0.3779 - val_accuracy: 0.9351\n",
            "Epoch 331/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3468 - accuracy: 0.9522 - val_loss: 0.3803 - val_accuracy: 0.9340\n",
            "Epoch 332/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3448 - accuracy: 0.9537 - val_loss: 0.3888 - val_accuracy: 0.9276\n",
            "Epoch 333/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3487 - accuracy: 0.9521 - val_loss: 0.3821 - val_accuracy: 0.9330\n",
            "Epoch 334/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3475 - accuracy: 0.9496 - val_loss: 0.3746 - val_accuracy: 0.9373\n",
            "Epoch 335/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3418 - accuracy: 0.9545 - val_loss: 0.3736 - val_accuracy: 0.9359\n",
            "Epoch 336/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3414 - accuracy: 0.9562 - val_loss: 0.3787 - val_accuracy: 0.9357\n",
            "Epoch 337/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3446 - accuracy: 0.9536 - val_loss: 0.3753 - val_accuracy: 0.9368\n",
            "Epoch 338/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3428 - accuracy: 0.9552 - val_loss: 0.3796 - val_accuracy: 0.9337\n",
            "Epoch 339/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3431 - accuracy: 0.9525 - val_loss: 0.3833 - val_accuracy: 0.9305\n",
            "Epoch 340/500\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3504 - accuracy: 0.9479 - val_loss: 0.3757 - val_accuracy: 0.9336\n",
            "Epoch 341/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3447 - accuracy: 0.9505 - val_loss: 0.3860 - val_accuracy: 0.9286\n",
            "Epoch 342/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3447 - accuracy: 0.9528 - val_loss: 0.3818 - val_accuracy: 0.9306\n",
            "Epoch 343/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3463 - accuracy: 0.9516 - val_loss: 0.3752 - val_accuracy: 0.9347\n",
            "Epoch 344/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3443 - accuracy: 0.9520 - val_loss: 0.3869 - val_accuracy: 0.9279\n",
            "Epoch 345/500\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3464 - accuracy: 0.9511 - val_loss: 0.3770 - val_accuracy: 0.9343\n",
            "Epoch 346/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3408 - accuracy: 0.9535 - val_loss: 0.3751 - val_accuracy: 0.9359\n",
            "Epoch 347/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3407 - accuracy: 0.9538 - val_loss: 0.3756 - val_accuracy: 0.9338\n",
            "Epoch 348/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3381 - accuracy: 0.9565 - val_loss: 0.3706 - val_accuracy: 0.9381\n",
            "Epoch 349/500\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3379 - accuracy: 0.9565 - val_loss: 0.3781 - val_accuracy: 0.9329\n",
            "Epoch 350/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3428 - accuracy: 0.9537 - val_loss: 0.3755 - val_accuracy: 0.9343\n",
            "Epoch 351/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3400 - accuracy: 0.9551 - val_loss: 0.3768 - val_accuracy: 0.9331\n",
            "Epoch 352/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3394 - accuracy: 0.9547 - val_loss: 0.3828 - val_accuracy: 0.9308\n",
            "Epoch 353/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3402 - accuracy: 0.9537 - val_loss: 0.3763 - val_accuracy: 0.9334\n",
            "Epoch 354/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3381 - accuracy: 0.9558 - val_loss: 0.3747 - val_accuracy: 0.9362\n",
            "Epoch 355/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3414 - accuracy: 0.9530 - val_loss: 0.3727 - val_accuracy: 0.9354\n",
            "Epoch 356/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3379 - accuracy: 0.9572 - val_loss: 0.3819 - val_accuracy: 0.9319\n",
            "Epoch 357/500\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3378 - accuracy: 0.9557 - val_loss: 0.3855 - val_accuracy: 0.9275\n",
            "Epoch 358/500\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3404 - accuracy: 0.9519 - val_loss: 0.3719 - val_accuracy: 0.9370\n",
            "Epoch 359/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3355 - accuracy: 0.9578 - val_loss: 0.3803 - val_accuracy: 0.9325\n",
            "Epoch 360/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3437 - accuracy: 0.9512 - val_loss: 0.3860 - val_accuracy: 0.9264\n",
            "Epoch 361/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3392 - accuracy: 0.9552 - val_loss: 0.3823 - val_accuracy: 0.9288\n",
            "Epoch 362/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3420 - accuracy: 0.9527 - val_loss: 0.3756 - val_accuracy: 0.9346\n",
            "Epoch 363/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3387 - accuracy: 0.9549 - val_loss: 0.3738 - val_accuracy: 0.9341\n",
            "Epoch 364/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3365 - accuracy: 0.9556 - val_loss: 0.3807 - val_accuracy: 0.9300\n",
            "Epoch 365/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3397 - accuracy: 0.9530 - val_loss: 0.3707 - val_accuracy: 0.9371\n",
            "Epoch 366/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3367 - accuracy: 0.9570 - val_loss: 0.3712 - val_accuracy: 0.9362\n",
            "Epoch 367/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3360 - accuracy: 0.9567 - val_loss: 0.3722 - val_accuracy: 0.9347\n",
            "Epoch 368/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3385 - accuracy: 0.9556 - val_loss: 0.3721 - val_accuracy: 0.9339\n",
            "Epoch 369/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3360 - accuracy: 0.9574 - val_loss: 0.3687 - val_accuracy: 0.9374\n",
            "Epoch 370/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3399 - accuracy: 0.9527 - val_loss: 0.3703 - val_accuracy: 0.9352\n",
            "Epoch 371/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3367 - accuracy: 0.9568 - val_loss: 0.3703 - val_accuracy: 0.9368\n",
            "Epoch 372/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3367 - accuracy: 0.9559 - val_loss: 0.3703 - val_accuracy: 0.9386\n",
            "Epoch 373/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3368 - accuracy: 0.9559 - val_loss: 0.3698 - val_accuracy: 0.9363\n",
            "Epoch 374/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3335 - accuracy: 0.9575 - val_loss: 0.3820 - val_accuracy: 0.9283\n",
            "Epoch 375/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3380 - accuracy: 0.9527 - val_loss: 0.3678 - val_accuracy: 0.9374\n",
            "Epoch 376/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3361 - accuracy: 0.9566 - val_loss: 0.3694 - val_accuracy: 0.9367\n",
            "Epoch 377/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3348 - accuracy: 0.9554 - val_loss: 0.3671 - val_accuracy: 0.9382\n",
            "Epoch 378/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3322 - accuracy: 0.9582 - val_loss: 0.3681 - val_accuracy: 0.9379\n",
            "Epoch 379/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3347 - accuracy: 0.9553 - val_loss: 0.3775 - val_accuracy: 0.9325\n",
            "Epoch 380/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3344 - accuracy: 0.9567 - val_loss: 0.3745 - val_accuracy: 0.9339\n",
            "Epoch 381/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3411 - accuracy: 0.9509 - val_loss: 0.3746 - val_accuracy: 0.9328\n",
            "Epoch 382/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3353 - accuracy: 0.9554 - val_loss: 0.3694 - val_accuracy: 0.9353\n",
            "Epoch 383/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3354 - accuracy: 0.9554 - val_loss: 0.3650 - val_accuracy: 0.9403\n",
            "Epoch 384/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3304 - accuracy: 0.9586 - val_loss: 0.3687 - val_accuracy: 0.9373\n",
            "Epoch 385/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3364 - accuracy: 0.9541 - val_loss: 0.3727 - val_accuracy: 0.9341\n",
            "Epoch 386/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3315 - accuracy: 0.9587 - val_loss: 0.3736 - val_accuracy: 0.9351\n",
            "Epoch 387/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3318 - accuracy: 0.9589 - val_loss: 0.3815 - val_accuracy: 0.9278\n",
            "Epoch 388/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3396 - accuracy: 0.9517 - val_loss: 0.3730 - val_accuracy: 0.9339\n",
            "Epoch 389/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3368 - accuracy: 0.9554 - val_loss: 0.3675 - val_accuracy: 0.9353\n",
            "Epoch 390/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3289 - accuracy: 0.9596 - val_loss: 0.3642 - val_accuracy: 0.9392\n",
            "Epoch 391/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3333 - accuracy: 0.9573 - val_loss: 0.3734 - val_accuracy: 0.9353\n",
            "Epoch 392/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3347 - accuracy: 0.9567 - val_loss: 0.3788 - val_accuracy: 0.9309\n",
            "Epoch 393/500\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3377 - accuracy: 0.9538 - val_loss: 0.3713 - val_accuracy: 0.9320\n",
            "Epoch 394/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3301 - accuracy: 0.9587 - val_loss: 0.3653 - val_accuracy: 0.9382\n",
            "Epoch 395/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3304 - accuracy: 0.9591 - val_loss: 0.3697 - val_accuracy: 0.9358\n",
            "Epoch 396/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3311 - accuracy: 0.9568 - val_loss: 0.3692 - val_accuracy: 0.9347\n",
            "Epoch 397/500\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3319 - accuracy: 0.9565 - val_loss: 0.3672 - val_accuracy: 0.9374\n",
            "Epoch 398/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3295 - accuracy: 0.9582 - val_loss: 0.3684 - val_accuracy: 0.9361\n",
            "Epoch 399/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3297 - accuracy: 0.9590 - val_loss: 0.3699 - val_accuracy: 0.9358\n",
            "Epoch 400/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3366 - accuracy: 0.9540 - val_loss: 0.3659 - val_accuracy: 0.9358\n",
            "Epoch 401/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3320 - accuracy: 0.9565 - val_loss: 0.3698 - val_accuracy: 0.9349\n",
            "Epoch 402/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3312 - accuracy: 0.9578 - val_loss: 0.3715 - val_accuracy: 0.9345\n",
            "Epoch 403/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3356 - accuracy: 0.9547 - val_loss: 0.3698 - val_accuracy: 0.9357\n",
            "Epoch 404/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3274 - accuracy: 0.9605 - val_loss: 0.3724 - val_accuracy: 0.9339\n",
            "Epoch 405/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3301 - accuracy: 0.9559 - val_loss: 0.3759 - val_accuracy: 0.9312\n",
            "Epoch 406/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3318 - accuracy: 0.9574 - val_loss: 0.3641 - val_accuracy: 0.9376\n",
            "Epoch 407/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3301 - accuracy: 0.9571 - val_loss: 0.3636 - val_accuracy: 0.9396\n",
            "Epoch 408/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3301 - accuracy: 0.9595 - val_loss: 0.3654 - val_accuracy: 0.9391\n",
            "Epoch 409/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3291 - accuracy: 0.9595 - val_loss: 0.3704 - val_accuracy: 0.9350\n",
            "Epoch 410/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3345 - accuracy: 0.9540 - val_loss: 0.3764 - val_accuracy: 0.9314\n",
            "Epoch 411/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3349 - accuracy: 0.9541 - val_loss: 0.3640 - val_accuracy: 0.9392\n",
            "Epoch 412/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3252 - accuracy: 0.9610 - val_loss: 0.3657 - val_accuracy: 0.9363\n",
            "Epoch 413/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3289 - accuracy: 0.9579 - val_loss: 0.3646 - val_accuracy: 0.9365\n",
            "Epoch 414/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3278 - accuracy: 0.9596 - val_loss: 0.3657 - val_accuracy: 0.9360\n",
            "Epoch 415/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3282 - accuracy: 0.9597 - val_loss: 0.3642 - val_accuracy: 0.9375\n",
            "Epoch 416/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3292 - accuracy: 0.9587 - val_loss: 0.3642 - val_accuracy: 0.9380\n",
            "Epoch 417/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3263 - accuracy: 0.9602 - val_loss: 0.3668 - val_accuracy: 0.9379\n",
            "Epoch 418/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3257 - accuracy: 0.9612 - val_loss: 0.3686 - val_accuracy: 0.9356\n",
            "Epoch 419/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3265 - accuracy: 0.9594 - val_loss: 0.3637 - val_accuracy: 0.9380\n",
            "Epoch 420/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3256 - accuracy: 0.9601 - val_loss: 0.3653 - val_accuracy: 0.9372\n",
            "Epoch 421/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3261 - accuracy: 0.9597 - val_loss: 0.3607 - val_accuracy: 0.9399\n",
            "Epoch 422/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3273 - accuracy: 0.9609 - val_loss: 0.3649 - val_accuracy: 0.9372\n",
            "Epoch 423/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3289 - accuracy: 0.9592 - val_loss: 0.3756 - val_accuracy: 0.9304\n",
            "Epoch 424/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3357 - accuracy: 0.9527 - val_loss: 0.3645 - val_accuracy: 0.9375\n",
            "Epoch 425/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3284 - accuracy: 0.9578 - val_loss: 0.3602 - val_accuracy: 0.9401\n",
            "Epoch 426/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3245 - accuracy: 0.9614 - val_loss: 0.3684 - val_accuracy: 0.9354\n",
            "Epoch 427/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3296 - accuracy: 0.9571 - val_loss: 0.3610 - val_accuracy: 0.9401\n",
            "Epoch 428/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3246 - accuracy: 0.9609 - val_loss: 0.3617 - val_accuracy: 0.9401\n",
            "Epoch 429/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3245 - accuracy: 0.9612 - val_loss: 0.3681 - val_accuracy: 0.9358\n",
            "Epoch 430/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3274 - accuracy: 0.9588 - val_loss: 0.3615 - val_accuracy: 0.9393\n",
            "Epoch 431/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3254 - accuracy: 0.9604 - val_loss: 0.3655 - val_accuracy: 0.9352\n",
            "Epoch 432/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3266 - accuracy: 0.9585 - val_loss: 0.3645 - val_accuracy: 0.9361\n",
            "Epoch 433/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3281 - accuracy: 0.9581 - val_loss: 0.3671 - val_accuracy: 0.9365\n",
            "Epoch 434/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3259 - accuracy: 0.9593 - val_loss: 0.3689 - val_accuracy: 0.9344\n",
            "Epoch 435/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3381 - accuracy: 0.9502 - val_loss: 0.3624 - val_accuracy: 0.9388\n",
            "Epoch 436/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3232 - accuracy: 0.9627 - val_loss: 0.3617 - val_accuracy: 0.9395\n",
            "Epoch 437/500\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3242 - accuracy: 0.9604 - val_loss: 0.3695 - val_accuracy: 0.9353\n",
            "Epoch 438/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3270 - accuracy: 0.9582 - val_loss: 0.3671 - val_accuracy: 0.9351\n",
            "Epoch 439/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3279 - accuracy: 0.9577 - val_loss: 0.3667 - val_accuracy: 0.9360\n",
            "Epoch 440/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3288 - accuracy: 0.9585 - val_loss: 0.3621 - val_accuracy: 0.9386\n",
            "Epoch 441/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3239 - accuracy: 0.9605 - val_loss: 0.3635 - val_accuracy: 0.9372\n",
            "Epoch 442/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3259 - accuracy: 0.9608 - val_loss: 0.3615 - val_accuracy: 0.9387\n",
            "Epoch 443/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3247 - accuracy: 0.9595 - val_loss: 0.3716 - val_accuracy: 0.9326\n",
            "Epoch 444/500\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3284 - accuracy: 0.9573 - val_loss: 0.3596 - val_accuracy: 0.9397\n",
            "Epoch 445/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3220 - accuracy: 0.9620 - val_loss: 0.3616 - val_accuracy: 0.9391\n",
            "Epoch 446/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3230 - accuracy: 0.9621 - val_loss: 0.3638 - val_accuracy: 0.9372\n",
            "Epoch 447/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3265 - accuracy: 0.9583 - val_loss: 0.3632 - val_accuracy: 0.9361\n",
            "Epoch 448/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3241 - accuracy: 0.9603 - val_loss: 0.3627 - val_accuracy: 0.9392\n",
            "Epoch 449/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3246 - accuracy: 0.9600 - val_loss: 0.3675 - val_accuracy: 0.9349\n",
            "Epoch 450/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3239 - accuracy: 0.9607 - val_loss: 0.3600 - val_accuracy: 0.9397\n",
            "Epoch 451/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3254 - accuracy: 0.9586 - val_loss: 0.3713 - val_accuracy: 0.9334\n",
            "Epoch 452/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3303 - accuracy: 0.9560 - val_loss: 0.3607 - val_accuracy: 0.9399\n",
            "Epoch 453/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3241 - accuracy: 0.9608 - val_loss: 0.3629 - val_accuracy: 0.9353\n",
            "Epoch 454/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3237 - accuracy: 0.9608 - val_loss: 0.3644 - val_accuracy: 0.9373\n",
            "Epoch 455/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3233 - accuracy: 0.9604 - val_loss: 0.3611 - val_accuracy: 0.9391\n",
            "Epoch 456/500\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.3228 - accuracy: 0.9613 - val_loss: 0.3586 - val_accuracy: 0.9404\n",
            "Epoch 457/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3232 - accuracy: 0.9617 - val_loss: 0.3618 - val_accuracy: 0.9394\n",
            "Epoch 458/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3254 - accuracy: 0.9598 - val_loss: 0.3624 - val_accuracy: 0.9374\n",
            "Epoch 459/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3259 - accuracy: 0.9590 - val_loss: 0.3640 - val_accuracy: 0.9377\n",
            "Epoch 460/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3240 - accuracy: 0.9595 - val_loss: 0.3761 - val_accuracy: 0.9303\n",
            "Epoch 461/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3327 - accuracy: 0.9536 - val_loss: 0.3607 - val_accuracy: 0.9383\n",
            "Epoch 462/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3200 - accuracy: 0.9627 - val_loss: 0.3587 - val_accuracy: 0.9386\n",
            "Epoch 463/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3233 - accuracy: 0.9619 - val_loss: 0.3598 - val_accuracy: 0.9374\n",
            "Epoch 464/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3189 - accuracy: 0.9612 - val_loss: 0.3604 - val_accuracy: 0.9394\n",
            "Epoch 465/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3223 - accuracy: 0.9608 - val_loss: 0.3623 - val_accuracy: 0.9374\n",
            "Epoch 466/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3236 - accuracy: 0.9599 - val_loss: 0.3578 - val_accuracy: 0.9404\n",
            "Epoch 467/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3230 - accuracy: 0.9607 - val_loss: 0.3742 - val_accuracy: 0.9311\n",
            "Epoch 468/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3288 - accuracy: 0.9560 - val_loss: 0.3567 - val_accuracy: 0.9421\n",
            "Epoch 469/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3215 - accuracy: 0.9606 - val_loss: 0.3639 - val_accuracy: 0.9354\n",
            "Epoch 470/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3264 - accuracy: 0.9583 - val_loss: 0.3627 - val_accuracy: 0.9376\n",
            "Epoch 471/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3236 - accuracy: 0.9594 - val_loss: 0.3586 - val_accuracy: 0.9406\n",
            "Epoch 472/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3253 - accuracy: 0.9593 - val_loss: 0.3627 - val_accuracy: 0.9375\n",
            "Epoch 473/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3205 - accuracy: 0.9641 - val_loss: 0.3674 - val_accuracy: 0.9346\n",
            "Epoch 474/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3234 - accuracy: 0.9607 - val_loss: 0.3668 - val_accuracy: 0.9353\n",
            "Epoch 475/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3259 - accuracy: 0.9592 - val_loss: 0.3568 - val_accuracy: 0.9409\n",
            "Epoch 476/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3198 - accuracy: 0.9629 - val_loss: 0.3591 - val_accuracy: 0.9389\n",
            "Epoch 477/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3198 - accuracy: 0.9624 - val_loss: 0.3692 - val_accuracy: 0.9326\n",
            "Epoch 478/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3266 - accuracy: 0.9574 - val_loss: 0.3596 - val_accuracy: 0.9392\n",
            "Epoch 479/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3196 - accuracy: 0.9631 - val_loss: 0.3569 - val_accuracy: 0.9397\n",
            "Epoch 480/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3190 - accuracy: 0.9642 - val_loss: 0.3663 - val_accuracy: 0.9373\n",
            "Epoch 481/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3215 - accuracy: 0.9610 - val_loss: 0.3609 - val_accuracy: 0.9392\n",
            "Epoch 482/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3199 - accuracy: 0.9632 - val_loss: 0.3557 - val_accuracy: 0.9411\n",
            "Epoch 483/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3183 - accuracy: 0.9625 - val_loss: 0.3666 - val_accuracy: 0.9336\n",
            "Epoch 484/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3230 - accuracy: 0.9603 - val_loss: 0.3595 - val_accuracy: 0.9382\n",
            "Epoch 485/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3203 - accuracy: 0.9623 - val_loss: 0.3574 - val_accuracy: 0.9401\n",
            "Epoch 486/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3222 - accuracy: 0.9603 - val_loss: 0.3616 - val_accuracy: 0.9350\n",
            "Epoch 487/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3198 - accuracy: 0.9626 - val_loss: 0.3599 - val_accuracy: 0.9364\n",
            "Epoch 488/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3178 - accuracy: 0.9642 - val_loss: 0.3756 - val_accuracy: 0.9286\n",
            "Epoch 489/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3250 - accuracy: 0.9589 - val_loss: 0.3557 - val_accuracy: 0.9418\n",
            "Epoch 490/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3189 - accuracy: 0.9630 - val_loss: 0.3774 - val_accuracy: 0.9293\n",
            "Epoch 491/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3278 - accuracy: 0.9551 - val_loss: 0.3608 - val_accuracy: 0.9382\n",
            "Epoch 492/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3175 - accuracy: 0.9625 - val_loss: 0.3619 - val_accuracy: 0.9360\n",
            "Epoch 493/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3206 - accuracy: 0.9609 - val_loss: 0.3585 - val_accuracy: 0.9387\n",
            "Epoch 494/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3190 - accuracy: 0.9628 - val_loss: 0.3593 - val_accuracy: 0.9385\n",
            "Epoch 495/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3193 - accuracy: 0.9625 - val_loss: 0.3627 - val_accuracy: 0.9372\n",
            "Epoch 496/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3197 - accuracy: 0.9615 - val_loss: 0.3631 - val_accuracy: 0.9361\n",
            "Epoch 497/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3208 - accuracy: 0.9615 - val_loss: 0.3600 - val_accuracy: 0.9384\n",
            "Epoch 498/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3205 - accuracy: 0.9612 - val_loss: 0.3581 - val_accuracy: 0.9388\n",
            "Epoch 499/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3208 - accuracy: 0.9612 - val_loss: 0.3606 - val_accuracy: 0.9372\n",
            "Epoch 500/500\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3163 - accuracy: 0.9636 - val_loss: 0.3567 - val_accuracy: 0.9392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f740c0ccd10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WXADMLDiKq6"
      },
      "source": [
        "#6 & 8 & 16\n",
        "cnnLayers = {}\n",
        "Model = {}\n",
        "Logs = {}\n",
        "inputs = keras.Input(shape=(noChannels,noSU,1),name='CostMatrix',dtype = tf.float32)\n",
        "\n",
        "for i in range(noSU):\n",
        "   \n",
        "  cnnLayers['L'+str(i)] = layers.Conv2D(10,(2,1),kernel_initializer='glorot_uniform',kernel_regularizer='l2', activation='relu')(inputs)\n",
        "  cnnLayers['L'+str(i)] = layers.Conv2D(16,(1,2),kernel_initializer='glorot_uniform',kernel_regularizer='l2',activation='relu')(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Conv2D(32,(2,1),kernel_initializer='glorot_uniform',kernel_regularizer='l2',activation='relu')(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Flatten()(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Dense(256,activation='relu',kernel_regularizer='l2',)(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Dense(512,activation='relu',kernel_regularizer='l2',)(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Dense(128,activation='relu',kernel_regularizer='l2',)(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['L'+str(i)] = layers.Dense(64,activation='relu',kernel_regularizer='l2',)(cnnLayers['L'+str(i)])\n",
        "  cnnLayers['Out'+str(i)] = layers.Dense(noChannels,kernel_regularizer='l2',activation='softmax')(cnnLayers['L'+str(i)])\n",
        "  \n",
        "  Model['model'+ str(i)] = keras.Model(inputs,cnnLayers['Out'+str(i)],name ='cnn'+str(i))\n",
        "  Model['model'+ str(i)].compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(lr=0.001),metrics= ['accuracy'])\n",
        "\n",
        "locals().update(Model)\n",
        "\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4Lm_LuK_InI"
      },
      "source": [
        "keras.utils.plot_model(model1,\"my_first_model.png\",show_shapes=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn9zg0mszdDi"
      },
      "source": [
        "\n",
        "for j in range(noSU):\n",
        "  #j = 4\n",
        "  checkpoint_path='/content/gdrive/My Drive/Research_/UnbalLSAPfeed4/t'+str(j)+'.ckpt'\n",
        "  checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "  # Create a callback that saves the model's weights\n",
        "  cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                save_weights_only=True,\n",
        "                                                verbose=1, period= 50)\n",
        "  Logs['log'+str(j)] = Model['model'+str(j)].fit(trainDataIn,decisionOut[:,:,j],epochs=500,callbacks=[cp_callback],\n",
        "                                                batch_size=1024,verbose=1,validation_split=0.2)\n",
        "  print(\"train \\t\"+ str(j) + \"\\t over\")\n",
        "\n",
        "locals().update(Logs)\n",
        "for j in range(noSU):\n",
        "  fig,axes= plt.subplots(1,2,figsize=(10,4))\n",
        "  axes[0].plot(Logs['log'+str(j)].history['accuracy'])\n",
        "  axes[1].plot(Logs['log'+str(j)].history['loss'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xubbgbjW3ei4"
      },
      "source": [
        "for j in range(noSU):\n",
        "  checkpoint_path='/content/gdrive/My Drive/Research_/UnbalLSAPfeed4/t'+str(j)+'.ckpt'\n",
        "  Model[\"model\"+str(j)].load_weights(checkpoint_path)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEsN7SASAs6D"
      },
      "source": [
        "Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjOnYG0redza",
        "outputId": "405edb75-b601-41f6-b64d-ea3928eba83e"
      },
      "source": [
        "testCount = 11111\n",
        "testData = 100*np.random.randint(1,prec,[testCount,noChannels,noSU])/prec\n",
        "decisionOutTest = np.zeros([testCount,noChannels,noSU])\n",
        "predictTest = np.zeros([testCount,noChannels,noSU])\n",
        "for idx in range(testCount):\n",
        "  decisionOutTest[idx,:,:],_ = decisionGen(testData[idx,:,:])\n",
        "  #predictTest[idx,:,:] = predictDecision(testData[idx,:,:])\n",
        "for g in range(noChannels):\n",
        "  Model[\"model\"+str(g)].evaluate(testData,decisionOutTest[:,:,g])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3685 - accuracy: 0.9396\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3661 - accuracy: 0.9356\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3591 - accuracy: 0.9394\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3611 - accuracy: 0.9375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8if-0b6bJZfS"
      },
      "source": [
        "Collision Avoidance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMpg1fDdfEC4"
      },
      "source": [
        "def PredictionGeneration(G):\n",
        "  predictt = collisionAvoid(G,predGen(G))\n",
        "  return predictt,np.sum(G*predictt)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GU9xgOWL6gc"
      },
      "source": [
        "#G = np.random.randint(1,10,[noChannels,noSU])\n",
        "def predGen(G):\n",
        "  testD = np.zeros([1,noChannels,noSU,1])\n",
        "  testD[0,:,:,0] = G\n",
        "  pred = {}\n",
        "  predict = []\n",
        "  for k in range(noSU):\n",
        "    pred[\"predict\"+str(k)] = Model[\"model\"+str(k)].predict(testD[:1,:,:,:])\n",
        "    predict.append(pred[\"predict\"+str(k)])\n",
        "  predict = np.asarray(predict).reshape([noChannels,noSU]).T \n",
        "  predict_ = (predict/np.max(predict,axis=0))\n",
        "  predict_[predict_<1]=0\n",
        "  #print(predict_)\n",
        "  return predict_\n",
        "#print(G,\"\\n\",predGen(G),\"\\n\",decisionGen(G))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrAYfiRkL6c6"
      },
      "source": [
        "def collisionAvoid(G,predict_):\n",
        "  jobmulti = np.where(np.sum(predict_,axis=1)>1)[0].tolist()\n",
        "  jobunasign = np.where(np.sum(predict_,axis=1)<1)[0].tolist()\n",
        "  if len(jobmulti)!=0:\n",
        "    for t in jobmulti:\n",
        "      cols = np.where(predict_[t,:]>0)[0].tolist()\n",
        "      iddx = np.where(G[t,:]==np.min(G[t,cols]))[0].tolist()[0]\n",
        "      predict_[t,:] = 0\n",
        "      predict_[t,iddx]=1\n",
        "    uncols = np.where(np.sum(predict_,axis=0)<1)[0].tolist()\n",
        "    if len(uncols)==1:\n",
        "      predict_[jobunasign[0],uncols[0]] =1\n",
        "    else:\n",
        "      P = np.random.randint(100,300,[noChannels,noSU])\n",
        "      for k in jobunasign:\n",
        "        P[k,uncols]=G[k,uncols]\n",
        "      newPredict = collisionAvoid2(jobunasign,uncols,P)\n",
        "      predict_ = predict_ + newPredict\n",
        "  return predict_"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17M65TqjWGdl"
      },
      "source": [
        "def collisionAvoid2(jobunasign,uncols,P):\n",
        "  newPred2 = predGen(P)\n",
        "  subPred = np.zeros([noChannels,noSU])\n",
        "  for k in jobunasign:\n",
        "      subPred[k,uncols]=newPred2[k,uncols]\n",
        "  multipleJobs = np.where(np.sum(subPred, axis=1)>1)[0].tolist()\n",
        "  unassignedJobs = np.where(np.sum(subPred, axis=1)==0)[0].tolist()\n",
        "  if len(multipleJobs)==0:\n",
        "    return subPred\n",
        "  else :\n",
        "    for t in multipleJobs:\n",
        "      cols = np.where(subPred[t,:]>0)[0].tolist()\n",
        "      iddx = np.where(P[t,:]==np.min(P[t,cols]))[0].tolist()[0]\n",
        "      subPred[t,:] = 0\n",
        "      subPred[t,iddx]=1\n",
        "    uncols2 = np.where(np.sum(subPred,axis=0)<1)[0].tolist()\n",
        "    uncols2 = list(set(uncols).intersection(uncols2))\n",
        "    unassignedJobs = list(set(unassignedJobs).intersection(jobunasign))\n",
        "    P_ = np.random.randint(400,900,[noChannels,noSU])\n",
        "    for k in unassignedJobs:\n",
        "      P_[k,uncols2]=P[k,uncols2]\n",
        "    newPredict = collisionAvoid2(unassignedJobs,uncols2,P_)\n",
        "    return subPred +newPredict"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UdrBfiLISOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7778691-d198-40ea-9b7f-e22c3884746a"
      },
      "source": [
        "testCountlsap = 2000\n",
        "testDatalsap = 100*np.random.randint(1,prec,[testCountlsap,noChannels,noSU])/prec\n",
        "decidedCostlsap = np.zeros(testCountlsap)\n",
        "maxCostlsap = np.zeros(testCountlsap)\n",
        "lsapCost = np.zeros(testCountlsap)\n",
        "lsapdec = np.zeros([testCountlsap,noChannels,noSU])\n",
        "decisionOutTestlsap =  np.zeros([testCountlsap,noChannels,noSU])\n",
        "wdecisionOutlsap = np.zeros([testCountlsap,noChannels,noSU])\n",
        "percentagedevlsap = np.zeros(testCountlsap)\n",
        "toclsap = time.time()\n",
        "for idx in range(testCountlsap):\n",
        "  if idx%500 ==0:\n",
        "    print(idx)\n",
        "  decisionOutTestlsap[idx,:,:],decidedCostlsap[idx] = decisionGen(testDatalsap[idx,:,:])\n",
        "  wdecisionOutlsap[idx,:,:],_ = decisionGen(np.max(testDatalsap[idx,:,:])-testDatalsap[idx,:,:])\n",
        "  maxCostlsap[idx] = np.sum(wdecisionOutlsap[idx,:,:]*testDatalsap[idx,:,:])\n",
        "  lsapdec[idx,:,:],lsapCost[idx] = PredictionGeneration(testDatalsap[idx,:,:])\n",
        "  #print(np.round(decidedCostlsap[idx],2),\"\\t\\t\\t\",np.round(lsapCost[idx],2),\"\\n\")\n",
        "  percentagedevlsap[idx] = (100/(maxCostlsap[idx]-decidedCostlsap[idx])) * (lsapCost[idx] - decidedCostlsap[idx])\n",
        "ticlsap = time.time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vtFKBgoIVt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc99feb5-8810-4c88-f8aa-aebdc4c2a8e5"
      },
      "source": [
        "avgdevlsap = np.mean(percentagedevlsap)\n",
        "print(avgdevlsap,(ticlsap-toclsap)/testCountlsap)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.9282723941928723 0.11986881113052368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK0w9HntIVqo"
      },
      "source": [
        "tic  = time.time()\n",
        "for i in range(20000):\n",
        "  a=1+2\n",
        "toc = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZGzY5vhIVjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabfe481-a493-4aa4-9155-5f6d075f846a"
      },
      "source": [
        "print(toclsap-ticlsap)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-4229.123147726059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ8Y3N_UIVhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45f17c7-4905-47bc-dbdf-754ff3a8c0f1"
      },
      "source": [
        "toc-tic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0030705928802490234"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUGuTDECIVeQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq_M8LxNIVat"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss7PHXB9IVXC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4yQVJDKCvRC"
      },
      "source": [
        "def predictDecision(G):\n",
        "  testD = np.zeros([1,noChannels,noSU,1])\n",
        "  testD[0,:,:,0] = G\n",
        "  pred = {}\n",
        "  predict = []\n",
        "  for k in range(noSU):\n",
        "    pred[\"predict\"+str(k)] = Model[\"model\"+str(k)].predict(testD[:1,:,:,:])\n",
        "    predict.append(pred[\"predict\"+str(k)])\n",
        "  predict = np.asarray(predict).reshape([noChannels,noSU]) \n",
        "  outdecision = decisionGen(testD[0,:,:,0])\n",
        "\n",
        "  predict_ = (predict/np.max(predict,axis=1))\n",
        "  predict_[predict_<1]=0\n",
        "  if(np.linalg.det(predict_) == 0 ):\n",
        "    jobmulti = np.where(np.sum(predict_,axis=1)>1)[0]\n",
        "    jobunasign = np.where(np.sum(predict_,axis=1)<1)[0]\n",
        "    costs = predict_*G\n",
        "    rowmulti = costs[jobmulti,:]\n",
        "    maxidx = np.argmax(rowmulti)\n",
        "    predict_[jobmulti,maxidx]=0\n",
        "    predict_[jobunasign,maxidx]=1\n",
        "  return predict_.T, np.sum((predict_.T)*G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIyQO7p9On93"
      },
      "source": [
        "for jf in range(1000):\n",
        "  G = 10*np.random.randint(1,prec,[noChannels,noSU])/prec\n",
        "  if (np.linalg.det(predictDecision(G)) == 0 ):\n",
        "    print(G,'\\n\\n',decisionGen(G),'\\n\\n', predictDecision(G),\"\\n\\n\",np.sum(predictDecision(G)*G))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "YkaFVzL9IisC",
        "outputId": "d62f291e-64c3-4c21-f0b1-62f9664ed40e"
      },
      "source": [
        "testCountlsap = 10\n",
        "testDatalsap = 10*np.random.randint(1,prec,[testCountlsap,noChannels,noSU])/prec\n",
        "decidedCostlsap = np.zeros(testCountlsap)\n",
        "maxCostlsap = np.zeros(testCountlsap)\n",
        "lsapCost = np.zeros(testCountlsap)\n",
        "lsapdec = np.zeros([testCountlsap,noChannels,noSU])\n",
        "decisionOutTestlsap =  np.zeros([testCountlsap,noChannels,noSU])\n",
        "wdecisionOutlsap = np.zeros([testCountlsap,noChannels,noSU])\n",
        "percentagedevlsap = np.zeros(testCountlsap)\n",
        "for idx in range(testCountlsap):\n",
        "  decisionOutTestlsap[idx,:,:],decidedCostlsap[idx] = decisionGen(testDatalsap[idx,:,:])\n",
        "  wdecisionOutlsap[idx,:,:],_ = decisionGen(np.max(testDatalsap[idx,:,:])-testDatalsap[idx,:,:])\n",
        "  maxCostlsap[idx] = np.sum(wdecisionOutlsap[idx,:,:]*testDatalsap[idx,:,:])\n",
        "  lsapdec[idx,:,:],lsapCost[idx] = predictDecision(testDatalsap[idx,:,:])\n",
        "  #print(np.round(decidedCostlsap[idx],2),\"\\t\\t\\t\",np.round(lsapCost[idx],2),\"\\n\")\n",
        "  percentagedevlsap[idx] = (100/(maxCostlsap[idx]-decidedCostlsap[idx])) * (lsapCost[idx] - decidedCostlsap[idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-9f0d406a5847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mwdecisionOutlsap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecisionGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestDatalsap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtestDatalsap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mmaxCostlsap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwdecisionOutlsap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtestDatalsap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mlsapdec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlsapCost\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictDecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestDatalsap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;31m#print(np.round(decidedCostlsap[idx],2),\"\\t\\t\\t\",np.round(lsapCost[idx],2),\"\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mpercentagedevlsap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxCostlsap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdecidedCostlsap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlsapCost\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdecidedCostlsap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-dc2faaccc400>\u001b[0m in \u001b[0;36mpredictDecision\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mrowmulti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjobmulti\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmaxidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrowmulti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpredict_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjobmulti\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mpredict_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjobunasign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpredict_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for axis 1 with size 6"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weKvKOSsmDjK"
      },
      "source": [
        "#G = GG\n",
        "#G = Gh\n",
        "#G = np.random.randint(1,10,[noChannels,noSU])\n",
        "print(G)\n",
        "testD = np.zeros([1,noChannels,noSU,1])\n",
        "testD[0,:,:,0] = G\n",
        "pred = {}\n",
        "predict = []\n",
        "for k in range(noSU):\n",
        "  pred[\"predict\"+str(k)] = Model[\"model\"+str(k)].predict(testD[:1,:,:,:])\n",
        "  predict.append(pred[\"predict\"+str(k)])\n",
        "  #print(predict,\"\\n\")\n",
        "predict = np.asarray(predict).reshape([noChannels,noSU]).T \n",
        "#print(predict)\n",
        "#print(np.max(predict,axis=0))\n",
        "predict_ = (predict/np.max(predict,axis=0))\n",
        "predict_[predict_<1]=0\n",
        "print(predict_,'\\n')\n",
        "jobmulti = np.where(np.sum(predict_,axis=1)>1)[0].tolist()\n",
        "jobunasign = np.where(np.sum(predict_,axis=1)<1)[0].tolist()\n",
        "for t in jobmulti:\n",
        "  cols = np.where(predict_[t,:]>0)[0].tolist()\n",
        "  iddx = np.where(G[t,:]==np.min(G[t,cols]))[0].tolist()[0]\n",
        "  print(iddx)\n",
        "  \n",
        "  #print(t,cols,iddx)\n",
        "  predict_[t,:] = 0\n",
        "  predict_[t,iddx]=1\n",
        "\n",
        "uncol = np.where(np.sum(predict_,axis=0)<1)[0].tolist()\n",
        "P = np.random.randint(50,150,[noChannels,noSU])\n",
        "for k in jobunasign:\n",
        "  P[k,uncol]=G[k,uncol]\n",
        "print(predict_)\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
        "'''\n",
        "\n",
        "if(np.linalg.det(predict_) == 0):\n",
        "  jobmulti = np.where(np.sum(predict_,axis=1)>1)[0]\n",
        "  print(jobmulti)\n",
        "  jobunasign = np.where(np.sum(predict_,axis=1)<1)[0]\n",
        "  print(jobunasign)\n",
        "  costs = predict_*G\n",
        "  rowmulti = costs[jobmulti,:]\n",
        "  print(rowmulti)\n",
        "  maxidx = np.argmax(rowmulti)\n",
        "  print(maxidx)\n",
        "  predict_[jobmulti,maxidx]=0\n",
        "  predict_[jobunasign,maxidx]=1\n",
        "print(predict_.T, np.sum((predict_)*G))\n",
        "\n",
        "print(predictDecision(G))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr_N6zWFIQnB"
      },
      "source": [
        "#OLD\n",
        "\n",
        "def predictDecision(G):\n",
        "  global count\n",
        "  #count =noChannels\n",
        "  testD = np.zeros([1,noChannels,noSU,1])\n",
        "  testD[0,:,:,0] = G\n",
        "  pred = {}\n",
        "  predict = []\n",
        "  for k in range(noSU):\n",
        "    pred[\"predict\"+str(k)] = Model[\"model\"+str(k)].predict(testD[:1,:,:,:])\n",
        "    predict.append(pred[\"predict\"+str(k)])\n",
        "  predict = np.asarray(predict).reshape([noChannels,noSU]).T\n",
        "  predict_ = (predict/np.max(predict,axis=0))\n",
        "  predict_[predict_<1]=0\n",
        "  jobmulti = np.where(np.sum(predict_,axis=1)>1)[0].tolist()\n",
        "  jobunasign = np.where(np.sum(predict_,axis=1)<1)[0].tolist()\n",
        "  if len(jobmulti)<1 :\n",
        "    print(\"no\",predict_,\"\\n\")\n",
        "    return predict_ , np.sum(predict_*G)\n",
        "  else :\n",
        "    for t in jobmulti:\n",
        "      cols = np.where(predict_[t,:]>0)[0].tolist()\n",
        "      iddx = np.where(G[t,:]==np.min(G[t,cols]))[0].tolist()[0]\n",
        "      predict_[t,:] = 0\n",
        "      predict_[t,iddx]=1\n",
        "    uncol = np.where(np.sum(predict_,axis=0)<1)[0].tolist()\n",
        "    P = np.random.randint(250,10000,[noChannels,noSU])\n",
        "    for k in jobunasign:\n",
        "      P[k,uncol]=G[k,uncol]\n",
        "    newPred,_ = predictDecision(P)\n",
        "    for k in jobunasign:\n",
        "      predict_[k,uncol]=newPred[k,uncol]\n",
        "    print(\"yes\",predict_,\"\\n\")\n",
        "    return predict_ , np.sum(predict_*G)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0O-4J97djAr",
        "outputId": "fb913815-38ac-4de2-a230-9f1f7ef36384"
      },
      "source": [
        "count = 0\n",
        "for d in range(2000):\n",
        "  G = 10*np.random.randint(1,prec,[noChannels,noSU])/prec\n",
        "  ppp ,_ = PredictionGeneration(G)\n",
        "  print(np.linalg.det(ppp))\n",
        "  if np.linalg.det(ppp)==0:\n",
        "    H = G\n",
        "    break\n",
        "  else:\n",
        "    count = count+1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJIjixDUq7Di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894420f6-0bb0-4a08-b339-4d176fc4f5e4"
      },
      "source": [
        "PredictionGeneration(G)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0.]]), 13.695)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKzyO4B_SafT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21451f12-1b70-4bd0-821d-2f40ad11cb09"
      },
      "source": [
        "print(G,\"\\n\",decisionGen(G))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.711 6.867 1.981 1.823 5.006 1.842]\n",
            " [7.353 3.318 7.653 9.018 0.666 5.859]\n",
            " [8.95  7.65  9.144 0.067 3.398 9.125]\n",
            " [8.659 7.774 5.399 1.816 7.379 7.524]\n",
            " [2.895 7.525 6.008 9.486 4.639 5.548]\n",
            " [1.543 6.344 0.451 3.379 5.1   9.625]] \n",
            " (array([[0., 0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1., 0.],\n",
            "       [0., 0., 0., 1., 0., 0.],\n",
            "       [0., 1., 0., 0., 0., 0.],\n",
            "       [1., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 1., 0., 0., 0.]]), 13.695)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asTUs0um6Akk",
        "outputId": "28bd9b78-3e34-4e4d-efba-7bfa56f3a1b4"
      },
      "source": [
        "predict_ = predGen(G)\n",
        "predict_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [1., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 1., 0.],\n",
              "       [0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFOk30OKjGFF",
        "outputId": "0993163a-f6fa-4ae2-90cd-15fde34ea567"
      },
      "source": [
        "print(predict_)\n",
        "jobmulti = np.where(np.sum(predict_,axis=1)>1)[0].tolist()\n",
        "print(jobmulti)\n",
        "jobunasign = np.where(np.sum(predict_,axis=1)<1)[0].tolist()\n",
        "print(jobunasign)\n",
        "if len(jobmulti)==0:\n",
        "  print(\"CAIF\",predict_)\n",
        "  #return predict_\n",
        "else:\n",
        "  for t in jobmulti:\n",
        "    cols = np.where(predict_[t,:]>0)[0].tolist()\n",
        "    iddx = np.where(G[t,:]==np.min(G[t,cols]))[0].tolist()[0]\n",
        "    predict_[t,:] = 0\n",
        "    predict_[t,iddx]=1\n",
        "  print(predict_)\n",
        "  uncols = np.where(np.sum(predict_,axis=0)<1)[0].tolist()\n",
        "  print(uncols)\n",
        "  P = np.random.randint(100,300,[noChannels,noSU])\n",
        "  for k in jobunasign:\n",
        "      P[k,uncols]=G[k,uncols]\n",
        "  newPredict = collisionAvoid2(jobunasign,uncols,P)\n",
        "  print(newPredict)\n",
        "  predict_ = predict_ + newPredict\n",
        "  print(predict_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [1. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[2, 3]\n",
            "[0, 5]\n",
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[1, 4]\n",
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]]\n",
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyVIVVCGoDyL",
        "outputId": "4bb79647-afa7-4d2d-bd00-348448adb16f"
      },
      "source": [
        "print(P,predGen(P))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[277   4 246 227   3 228]\n",
            " [132 101 232 199 252 278]\n",
            " [268 232 291 201 132 158]\n",
            " [116 136 249 266 119 191]\n",
            " [137 195 193 251 226 214]\n",
            " [111   2 274 292   9 240]] [[0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdiydj7rkGQY",
        "outputId": "6fcd14cd-b9e5-49ea-93fc-64547b34f73d"
      },
      "source": [
        "newPred2 = predGen(P)\n",
        "print(newPred2)\n",
        "subPred = np.zeros([noChannels,noSU])\n",
        "for k in jobunasign:\n",
        "  subPred[k,uncols]=newPred2[k,uncols]\n",
        "print(subPred)\n",
        "multipleJobs = np.where(np.sum(subPred, axis=1)>1)[0].tolist()\n",
        "print(multipleJobs)\n",
        "unassignedJobs = np.where(np.sum(subPred, axis=1)==0)[0].tolist()\n",
        "print(unassignedJobs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[[0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[0]\n",
            "[1, 2, 3, 4, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s66ihfYk7_7",
        "outputId": "dc17714c-747b-4373-929a-3a5e1a2ef9a4"
      },
      "source": [
        "for t in multipleJobs:\n",
        "  cols = np.where(subPred[t,:]>0)[0].tolist()\n",
        "  iddx = np.where(P[t,:]==np.min(P[t,cols]))[0].tolist()[0]\n",
        "  subPred[t,:] = 0\n",
        "  subPred[t,iddx]=1\n",
        "print(subPred)\n",
        "uncols2 = np.where(np.sum(subPred,axis=0)<1)[0].tolist()\n",
        "uncols2 = list(set(uncols).intersection(uncols2))\n",
        "print(uncols2)\n",
        "unassignedJobs = list(set(unassignedJobs).intersection(jobunasign))\n",
        "print(unassignedJobs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[1]\n",
            "[5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAN9_F-tl8s8"
      },
      "source": [
        "P_ = np.random.randint(400,900,[noChannels,noSU])\n",
        "for k in unassignedJobs:\n",
        "  P_[k,uncols2]=P[k,uncols2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpT83xJ-mBs9",
        "outputId": "2789b89d-b936-4fba-9b6e-9d75d4291f5f"
      },
      "source": [
        "collisionAvoid2(unassignedJobs,uncols2,P_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBOSiYhCBlxU",
        "outputId": "f24eca38-157c-450a-cbd2-a19ed727835d"
      },
      "source": [
        "predict_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsFSxJRlfoHi",
        "outputId": "0716490c-fc4e-4e2e-967b-0f5eeb5d1414"
      },
      "source": [
        "newPredict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL8gAl08jGCT",
        "outputId": "29577bf6-de5e-49a8-d18e-23ecacee935a"
      },
      "source": [
        "  for t in jobmulti:\n",
        "    cols = np.where(predict_[t,:]>0)[0].tolist()\n",
        "    print(cols)\n",
        "    iddx = np.where(G[t,:]==np.min(G[t,cols]))[0].tolist()[0]\n",
        "    print(iddx)\n",
        "    predict_[t,:] = 0\n",
        "    predict_[t,iddx]=1\n",
        "    print(predict_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "0\n",
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[2]\n",
            "2\n",
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw_k-UB5jGAR",
        "outputId": "fc5263de-394f-4c6f-ad88-dcd59ed11c38"
      },
      "source": [
        "P"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[279, 210, 295, 151],\n",
              "       [208, 113, 177, 281],\n",
              "       [286, 289, 297, 126],\n",
              "       [  6, 226, 226, 140]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rG9TajTjF94"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsQ7Fm-UjF7q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JuER6OWjF5e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5Fvf3y1jF2y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suBd194yjFsR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bbu92c7FGrN"
      },
      "source": [
        "def predictDecision(G):\n",
        "  global count\n",
        "  #count =noChannels\n",
        "  testD = np.zeros([1,noChannels,noSU,1])\n",
        "  testD[0,:,:,0] = G\n",
        "  pred = {}\n",
        "  predict = []\n",
        "  for k in range(noSU):\n",
        "    pred[\"predict\"+str(k)] = Model[\"model\"+str(k)].predict(testD[:1,:,:,:])\n",
        "    predict.append(pred[\"predict\"+str(k)])\n",
        "  predict = np.asarray(predict).reshape([noChannels,noSU]).T\n",
        "  predict_ = (predict/np.max(predict,axis=0))\n",
        "  predict_[predict_<1]=0\n",
        "  jobmulti = np.where(np.sum(predict_,axis=1)>1)[0].tolist()\n",
        "  print(len(jobmulti))\n",
        "  jobunasign = np.where(np.sum(predict_,axis=1)<1)[0].tolist()\n",
        "  if len(jobmulti)<2 :\n",
        "    print(\"no\",predict_,\"\\n\")\n",
        "    return predict_ , np.sum(predict_*G)\n",
        "  else :\n",
        "    for t in jobmulti:\n",
        "      cols = np.where(predict_[t,:]>0)[0].tolist()\n",
        "      iddx = np.where(G[t,:]==np.min(G[t,cols]))[0].tolist()[0]\n",
        "      predict_[t,:] = 0\n",
        "      predict_[t,iddx]=1\n",
        "    uncol = np.where(np.sum(predict_,axis=0)<1)[0].tolist()\n",
        "    P = np.random.randint(10,30,[noChannels,noSU])\n",
        "    for k in jobunasign:\n",
        "      P[k,uncol]=G[k,uncol]\n",
        "    newPred,_ = predictDecision(P)\n",
        "    for k in jobunasign:\n",
        "      predict_[k,uncol]=newPred[k,uncol]\n",
        "    print(\"yes\",predict_,\"\\n\")\n",
        "    return predict_ , np.sum(predict_*G)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVDGYovGGZRP",
        "outputId": "b8b6d708-47b3-4592-9224-285f9b48080f"
      },
      "source": [
        "print(P)\n",
        "testD = np.zeros([1,noChannels,noSU,1])\n",
        "testD[0,:,:,0] = P\n",
        "pred = {}\n",
        "predict = []\n",
        "for k in range(noSU):\n",
        "  pred[\"predict\"+str(k)] = Model[\"model\"+str(k)].predict(testD[:1,:,:,:])\n",
        "  predict.append(pred[\"predict\"+str(k)])\n",
        "  #print(predict,\"\\n\")\n",
        "predict = np.asarray(predict).reshape([noChannels,noSU]).T \n",
        "#print(predict)\n",
        "#print(np.max(predict,axis=0))\n",
        "predict_ = (predict/np.max(predict,axis=0))\n",
        "predict_[predict_<1]=0\n",
        "print(np.round(predict_,2),decisionGen(testD[0,:,:,0]))\n",
        "jobmulti = np.where(np.sum(predict_,axis=1)>1)[0].tolist()\n",
        "jobunasign = np.where(np.sum(predict_,axis=1)<1)[0].tolist()\n",
        "for t in jobmulti:\n",
        "  cols = np.where(predict_[t,:]>0)[0].tolist()\n",
        "  iddx = np.where(P[t,:]==np.min(P[t,cols]))[0].tolist()[0]\n",
        "  print(iddx)\n",
        "  \n",
        "  #print(t,cols,iddx)\n",
        "  predict_[t,:] = 0\n",
        "  predict_[t,iddx]=1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[969 653 736 771 631 736 503 530]\n",
            " [725 626 659 525 623 961 763 737]\n",
            " [669   2 510   7 769   9   8 997]\n",
            " [824 608 713 838 937 577 917 669]\n",
            " [805   7 542   8 933   7   6 956]\n",
            " [735 863 761 854 915 807 561 814]\n",
            " [601   2 614   6 620   7   4 753]\n",
            " [729   1 865   1 702   4   3 896]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 1. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 1. 0. 1. 1. 0.]] (array([[0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "       [0., 0., 0., 1., 0., 0., 0., 0.]]), 2615.0)\n",
            "4\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pSii5loETJO"
      },
      "source": [
        "def predictDecision(G):\n",
        "  global count\n",
        "  count =20\n",
        "  testD = np.zeros([1,noChannels,noSU,1])\n",
        "  testD[0,:,:,0] = G\n",
        "  pred = {}\n",
        "  predict = []\n",
        "  for k in range(noSU):\n",
        "    pred[\"predict\"+str(k)] = Model[\"model\"+str(k)].predict(testD[:1,:,:,:])\n",
        "    predict.append(pred[\"predict\"+str(k)])\n",
        "  predict = np.asarray(predict).reshape([noChannels,noSU]).T\n",
        "  predict_ = (predict/np.max(predict,axis=0))\n",
        "  predict_[predict_<1]=0\n",
        "  jobmulti = np.where(np.sum(predict_,axis=1)>1)[0].tolist()\n",
        "  jobunasign = np.where(np.sum(predict_,axis=1)<1)[0].tolist()\n",
        "  if len(jobmulti)<1 and (count!=0):\n",
        "    return predict_ , np.sum(predict_*G)\n",
        "  elif (count!=0):\n",
        "    for t in jobmulti:\n",
        "      cols = np.where(predict_[t,:]>0)[0].tolist()\n",
        "      iddx = np.where(G[t,:]==np.min(G[t,cols]))[0].tolist()[0]\n",
        "      predict_[t,:] = 0\n",
        "      predict_[t,iddx]=1\n",
        "    uncol = np.where(np.sum(predict_,axis=0)<1)[0].tolist()\n",
        "    P = np.full([noChannels,noSU],2*np.max(G))\n",
        "    for k in jobunasign:\n",
        "      P[k,uncol]=G[k,uncol]\n",
        "    newPred,_ = predictDecision(P)\n",
        "    for k in jobunasign:\n",
        "      predict_[k,uncol]=newPred[k,uncol]\n",
        "    count = count - 1\n",
        "    print(count)\n",
        "    return predict_ , np.sum(predict_*G)\n",
        "  else :\n",
        "    return predictDecision(10*np.random.randint(1,prec,[testCountlsap,noChannels,noSU])/prec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3JAo3T4DJgz"
      },
      "source": [
        "P = np.full([noChannels,noSU],np.inf)\n",
        "for k in jobunasign:\n",
        "  P[k,uncol]=G[k,uncol]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7MDf5zSAIML",
        "outputId": "5853d24c-5ea7-4260-9404-0df541a7095e"
      },
      "source": [
        "predP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[inf, inf, inf, inf, inf],\n",
              "       [inf, inf, inf, inf, inf],\n",
              "       [inf, inf, inf, inf, inf],\n",
              "       [ 8., inf, inf, inf,  9.],\n",
              "       [ 9., inf, inf, inf,  4.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf3l_FyFAhN1",
        "outputId": "c52e4782-11f0-4e84-9c99-1e6ff5741c9b"
      },
      "source": [
        "H"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[inf, inf, inf, inf, inf],\n",
              "       [inf, inf, inf, inf, inf],\n",
              "       [inf, inf, inf, inf, inf],\n",
              "       [inf, inf, inf, inf, inf],\n",
              "       [inf, inf, inf, inf, inf]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXvVMmdRJWcG"
      },
      "source": [
        "truCost = 10*np.random.randint(1,prec,[noChannels,noSU])/prec\n",
        "initCost = np.zeros([noChannels,noSU])\n",
        "newCost = initCost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCXK_MeqQQY2"
      },
      "source": [
        "initCost = np.zeros([noChannels,noSU])\n",
        "newCost = initCost\n",
        "itr=10\n",
        "while (itr!=0):\n",
        "  decision = predictDecision(newCost)\n",
        "  mask = np.ones([noChannels,noSU])- decision\n",
        "  noisyCost = np.zeros([noChannels,noSU])\n",
        "  print(np.round(newCost,2),\"\\n\\n--Iteration-- \\n\\n\", decision)\n",
        "  for itt in range(100):\n",
        "    temp = truCost + np.random.normal(0,2,[noChannels,noSU])\n",
        "    noisyCost = temp + noisyCost\n",
        "  noisyCost = noisyCost/100\n",
        "  newCost = decision*noisyCost + mask*newCost\n",
        "  itr = itr-1\n",
        "print(np.round(truCost,2),'\\n\\n',np.round(newCost,2),'\\n\\n')\n",
        "print(decisionGen(truCost),'\\n\\n',decisionGen(newCost),'\\n\\n')\n",
        "print(predictDecision(truCost),'\\n\\n',predictDecision(newCost),'\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YajuIuzYmwzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39f654a-f97b-4753-f747-d1532674df3b"
      },
      "source": [
        "allcomb = []\n",
        "for r in range(noChannels):\n",
        "  for comb in combinations(channelindices,r):\n",
        "    allcomb.append(comb)\n",
        "print(H,\"\\n\\n\",outdecision,'\\n\\n',np.round(predict,2).T,'\\n\\n',predict_.T,np.sum(predict_*H))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[95 43 72 24 55]\n",
            " [ 9  5 76 47 13]\n",
            " [50 26 69 25 57]\n",
            " [30 14 66 66 14]\n",
            " [26 89 30 53 45]] \n",
            "\n",
            " (array([[0., 0., 0., 1., 0.],\n",
            "       [1., 0., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 1., 0., 0.]]), 103.0) \n",
            "\n",
            " [[0.   0.04 0.   0.97 0.01]\n",
            " [0.9  0.21 0.   0.   0.1 ]\n",
            " [0.01 0.23 0.   0.03 0.  ]\n",
            " [0.06 0.52 0.   0.   0.9 ]\n",
            " [0.03 0.   1.   0.   0.  ]] \n",
            "\n",
            " [[0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]] 259.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QMvD2m9p4Ry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319794ae-24fc-43a4-f9ed-c75997991467"
      },
      "source": [
        "testCount = 11111\n",
        "testData = 10*np.random.randint(1,prec,[testCount,noChannels,noSU])/prec\n",
        "decisionOutTest = np.zeros([testCount,noChannels,noSU])\n",
        "predictTest = np.zeros([testCount,noChannels,noSU])\n",
        "for idx in range(testCount):\n",
        "  decisionOutTest[idx,:,:],_ = decisionGen(testData[idx,:,:])\n",
        "  #predictTest[idx,:,:] = predictDecision(testData[idx,:,:])\n",
        "for g in range(noChannels):\n",
        "  Model[\"model\"+str(g)].evaluate(testData,decisionOutTest[:,:,g])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "348/348 [==============================] - 32s 2ms/step - loss: 0.5426 - accuracy: 0.8724\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5503 - accuracy: 0.8627\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5516 - accuracy: 0.8647\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5446 - accuracy: 0.8710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0-xiFTXjtVo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}